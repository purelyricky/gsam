{
  "_comment": "GSAM FiNER experiment using a self-hosted vLLM model on Modal.com. Before running: (1) modal deploy modal_vllm_server.py, (2) set MODAL_BASE_URL and MODAL_API_KEY env vars, (3) set generator/reflector/curator_model to the model you deployed (e.g. 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B' or the 'llm' alias).",
  "experiment_name": "gsam_finer_online_modal",
  "system": "gsam",
  "task_name": "finer",
  "mode": "online",
  "api_provider": "modal",
  "generator_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
  "reflector_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
  "curator_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
  "num_epochs": 1,
  "max_num_rounds": 3,
  "curator_frequency": 1,
  "online_eval_frequency": 15,
  "save_steps": 50,
  "max_tokens": 4096,
  "playbook_token_budget": 80000,
  "test_workers": 10,
  "taxonomy_path": "./eval/finance/data/xbrl_taxonomy.json",
  "merge_threshold": 0.9,
  "retrieval_depth": 2,
  "prune_frequency": 50,
  "no_ontology": false,
  "no_failure_cascades": false,
  "embedding_only_retrieval": false,
  "untyped_edges": false
}
